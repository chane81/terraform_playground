## Create default rules for monitoring the cluster
##
defaultRules:
  create: true

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:
  ## Deploy alertmanager
  ##
  enabled: true

  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
    receivers:
      - name: 'null'
    templates:
      - '/etc/alertmanager/config/*.tmpl'

  ## Configuration for Alertmanager service
  ##
  service:
    ## Service type
    ##
    type: NodePort

  ## Configuration for creating a separate Service for each statefulset Alertmanager replica
  ##
  servicePerReplica:
    ## Service type
    ##
    type: NodePort

## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
##
grafana:
  enabled: false

## Component scraping the kube api server
##
kubeApiServer:
  enabled: true

## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
  enabled: true
  namespace: kube-system

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: true

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: true

## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false

## Component scraping etcd
##
kubeEtcd:
  enabled: true

## Component scraping kube scheduler
##
kubeScheduler:
  enabled: true

## Component scraping kube proxy
##
kubeProxy:
  enabled: true

## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: true

## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: true

## Manages Prometheus and Alertmanager components
##
prometheusOperator:
  enabled: true

  ## Configuration for Prometheus operator service
  ##
  service:
    ## Service type
    ## NodePort, ClusterIP, LoadBalancer
    ##
    type: NodePort

  ## Resource limits & requests
  ##
  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 100m
      memory: 100Mi

  ## Prometheus-config-reloader
  ##
  prometheusConfigReloader:
    # resource config for prometheusConfigReloader
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
      limits:
        cpu: 200m
        memory: 50Mi

## Deploy a Prometheus instance
##
prometheus:
  enabled: true

  ## Configuration for Prometheus service
  ##
  service:
    ## Service type
    ##
    type: NodePort

  ## Configuration for creating a separate Service for each statefulset Prometheus replica
  ##
  servicePerReplica:
    ## Service type
    ##
    type: NodePort

  ingress:
    enabled: true

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    ingressClassName: alb

    annotations:
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/backend-protocol: HTTP

    labels: {}

    ## Hostnames.
    ## Must be provided if Ingress is enabled.
    ##
    # hosts:
    #   - prometheus.domain.com
    hosts: []

    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
    ##
    paths:
      - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for Prometheus Ingress
    ## Secret must be manually created in the namespace
    ##
    tls:
      []
      # - secretName: prometheus-general-tls
      #   hosts:
      #     - prometheus.example.com

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:
    ## How long to retain metrics
    ## 데이터유지 기간 - 3개월
    retention: 90d

    ## Maximum size of metrics
    ##
    retentionSize: '200GB'

    ## Number of replicas of each shard to deploy for a Prometheus deployment.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ##
    replicas: 1

    ## Log level for Prometheus be configured in
    ##
    logLevel: info

    ## Log format for Prometheus be configured in
    ##
    logFormat: logfmt

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    storageSpec:
      ## Using PersistentVolumeClaim
      ##
      volumeClaimTemplate:
        spec:
          storageClassName: awsElasticBlockStore
          accessModes: ['ReadWriteOnce']
          resources:
            requests:
              storage: 200Gi
          selector: {}

    # Additional volumes on the output StatefulSet definition.
    volumes: []

    # Additional VolumeMounts on the output StatefulSet definition.
    volumeMounts: []
